from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import time
import os

# RAG KÃ¼tÃ¼phaneleri
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient, models  # <--- DÃœZELTME BURADA: 'models' EKLENDÄ°
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# --- BAÅžLANGIÃ‡ AYARLARI ---
print("ðŸš€ NexusAI API BaÅŸlatÄ±lÄ±yor...")

app = FastAPI(title="NexusAI Engine", version="1.0")

# 1. Modelleri ve VeritabanÄ±nÄ± HafÄ±zaya YÃ¼kle
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Qdrant istemcisini baÅŸlat
client = QdrantClient(path="qdrant_db")
collection_name = "my_documents"

# --- KOLEKSÄ°YON KONTROLÃœ ---
# EÄŸer koleksiyon yoksa, boÅŸ bir tane oluÅŸtur (Hata vermemesi iÃ§in)
if not client.collection_exists(collection_name):
    print(f"âš ï¸ UyarÄ±: '{collection_name}' bulunamadÄ±. BoÅŸ olarak oluÅŸturuluyor...")
    client.create_collection(
        collection_name=collection_name,
        vectors_config=models.VectorParams(
            size=384, # all-MiniLM-L6-v2 modeli iÃ§in boyut 384'tÃ¼r.
            distance=models.Distance.COSINE
        )
    )

# Vector Store BaÄŸlantÄ±sÄ±
vector_store = QdrantVectorStore(
    client=client,
    collection_name=collection_name,
    embedding=embedding_model,
)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# LLM AyarlarÄ±
ollama_host = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434") 
# Not: Docker iÃ§inden localhost'a eriÅŸmek iÃ§in 'host.docker.internal' kullanmak daha gÃ¼venlidir.

llm = ChatOllama(
    model="llama3.2", 
    temperature=0.3,
    base_url=ollama_host,
    repeat_penalty=1.2,
    top_k=50,
    top_p=0.9
)

# 2. Zinciri (Chain) HazÄ±rla
prompt = ChatPromptTemplate.from_template("""
Sen TÃ¼rkÃ§e konuÅŸan profesyonel bir yapay zeka asistanÄ±sÄ±n.
Kurallar:
1. CevabÄ± MUTLAKA TÃ¼rkÃ§e ver.
2. BaÄŸlam dÄ±ÅŸÄ±na Ã§Ä±kma.
3. EÄŸer baÄŸlamda bilgi yoksa "Bu konuda bilgim yok" de.

<BaÄŸlam>
{context}
</BaÄŸlam>

Soru: {input}
""")

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

print("âœ… Sistem HazÄ±r! Ä°stek bekleniyor...")

# --- API ENDPOINTLERÄ° ---

class QueryRequest(BaseModel):
    question: str

@app.post("/ask")
def ask_question(request: QueryRequest):
    try:
        start_time = time.time()
        
        # Zinciri Ã§alÄ±ÅŸtÄ±r
        response = rag_chain.invoke({"input": request.question})
        
        duration = time.time() - start_time
        
        # KaynaklarÄ± temizle
        sources = []
        if "context" in response:
            for doc in response["context"]:
                sources.append(doc.page_content[:100].replace("\n", " ") + "...")

        return {
            "answer": response["answer"],
            "sources": sources,
            "processing_time": f"{duration:.2f} sn"
        }
        
    except Exception as e:
        print(f"HATA OLUÅžTU: {str(e)}") # Konsola hatayÄ± bas
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
def read_root():
    return {"status": "NexusAI Engine is Running ðŸš€"}