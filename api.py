from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import time

# RAG KÃ¼tÃ¼phaneleri
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# --- BAÅžLANGIÃ‡ AYARLARI (Sadece 1 kere Ã§alÄ±ÅŸÄ±r) ---
print("ðŸš€ NexusAI API BaÅŸlatÄ±lÄ±yor...")

app = FastAPI(title="NexusAI Engine", version="1.0")

# 1. Modelleri ve VeritabanÄ±nÄ± HafÄ±zaya YÃ¼kle (Global DeÄŸiÅŸkenler)
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
client = QdrantClient(path="./qdrant_db")
vector_store = QdrantVectorStore(
    client=client,
    collection_name="my_documents",
    embedding=embedding_model,
)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

import os

# YENÄ° HALÄ°:
# EÄŸer 'OLLAMA_HOST' diye bir Ã§evre deÄŸiÅŸkeni varsa onu kullan, yoksa varsayÄ±lanÄ± kullan.
ollama_host = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

llm = ChatOllama(
    model="llama3.2", 
    temperature=0.3,
    base_url=ollama_host,
    repeat_penalty=1.2,  # <-- SÄ°HÄ°RLÄ° DOKUNUÅž: TekrarÄ± cezalandÄ±rÄ±r
    top_k=50,            # <-- Kelime havuzunu sÄ±nÄ±rlar (daha mantÄ±klÄ± cÃ¼mleler)
    top_p=0.9            # <-- OlasÄ±lÄ±k filtresi
)

# 2. Zinciri (Chain) HazÄ±rla
prompt = ChatPromptTemplate.from_template("""
Sen TÃ¼rkÃ§e konuÅŸan profesyonel bir yapay zeka asistanÄ±sÄ±n.
Kurallar:
1. CevabÄ± MUTLAKA TÃ¼rkÃ§e ver.
2. BaÄŸlam dÄ±ÅŸÄ±na Ã§Ä±kma.

<BaÄŸlam>
{context}
</BaÄŸlam>

Soru: {input}
""")

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

print("âœ… Sistem HazÄ±r! Ä°stek bekleniyor...")

# --- API ENDPOINTLERÄ° ---

# Ä°stek Modeli (Gelen verinin formatÄ±)
class QueryRequest(BaseModel):
    question: str

@app.post("/ask")
def ask_question(request: QueryRequest):
    try:
        start_time = time.time()
        
        # Zinciri Ã§alÄ±ÅŸtÄ±r
        response = rag_chain.invoke({"input": request.question})
        
        duration = time.time() - start_time
        
        # KaynaklarÄ± temizle
        sources = []
        for doc in response["context"]:
            sources.append(doc.page_content[:100].replace("\n", " ") + "...")

        return {
            "answer": response["answer"],
            "sources": sources,
            "processing_time": f"{duration:.2f} sn"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
def read_root():
    return {"status": "NexusAI Engine is Running ðŸš€"}